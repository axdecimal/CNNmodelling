{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxo.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1vpcxJTe6EDjfHEz0DEZtBfPvLd-VH_UK",
      "authorship_tag": "ABX9TyPd8dSxqwHJn2QXxVK6TtNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axdecimal/CNNmodelling/blob/master/Toxo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi2iZ1_XwXwU",
        "outputId": "23e4ec4d-a404-46ff-a046-7d7f49668622"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go7SLbOWwnIV"
      },
      "source": [
        "zip_adres = \"/content/drive/MyDrive/Colab Notebook/Data2/toxo.zip\"\n",
        "!cp \"{zip_adres}\" ."
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpimTKpyDxis"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import math\n",
        "\n",
        "m = 2\n",
        "\n",
        "\n",
        "def initialize_U(num_data, num_cluster):\n",
        "    U = []\n",
        "    for data in range(num_data):\n",
        "        u_data = []\n",
        "        sum_random = 0.0\n",
        "        for cluster in range(num_cluster):\n",
        "            u_data.append(random.random())\n",
        "            sum_random += u_data[-1]\n",
        "        for cluster in range(num_cluster):\n",
        "            u_data[cluster] /= sum_random\n",
        "        U.append(u_data)\n",
        "\n",
        "    return U\n",
        "\n",
        "\n",
        "def initialize_C(data, U):\n",
        "    \"\"\"\n",
        "    :param data: features of the data\n",
        "    :param U:\n",
        "    :param m:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_cluster = len(U[0])\n",
        "    C = []\n",
        "    for j in range(num_cluster):\n",
        "        current_cluster_center = []\n",
        "        for i in range(len(data[0])):\n",
        "            dummy_sum_num = 0.0\n",
        "            dummy_sum_dum = 0.0\n",
        "            for k in range(len(data)):\n",
        "                # 分子\n",
        "                dummy_sum_num += (U[k][j] ** m) * data[k][i]\n",
        "                # 分母\n",
        "                dummy_sum_dum += (U[k][j] ** m)\n",
        "            # 第i列的聚类中心\n",
        "            current_cluster_center.append(dummy_sum_num / dummy_sum_dum)\n",
        "            # 第j簇的所有聚类中心\n",
        "        C.append(current_cluster_center)\n",
        "    return C\n",
        "\n",
        "\n",
        "def calculate_J(data, U, C):\n",
        "    \"\"\"\n",
        "    计算目标函数J\n",
        "    :param data:\n",
        "    :param C:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    J = 0.0\n",
        "    for num_C in range(len(C)):\n",
        "        for num_data in range(len(data)):\n",
        "            J += (np.power(U[num_data][num_C], m) * np.square(np.subtract(C[num_C], data[num_data])))\n",
        "    return J\n",
        "\n",
        "\n",
        "def distance(point, center):\n",
        "    \"\"\"\n",
        "    该函数计算2点之间的距离（作为列表）。我们指欧几里德距离。        闵可夫斯基距离\n",
        "    \"\"\"\n",
        "    if len(point) != len(center):\n",
        "        return -1\n",
        "    dummy = 0.0\n",
        "    for i in range(0, len(point)):\n",
        "        dummy += abs(point[i] - center[i]) ** 2\n",
        "    return math.sqrt(dummy)\n",
        "\n",
        "\n",
        "def end_conditon(U, U_old):\n",
        "    \"\"\"\n",
        "    结束条件。当U矩阵随着连续迭代停止变化时，触发结束\n",
        "    \"\"\"\n",
        "    epsm = 0.00000001\n",
        "    for i in range(0, len(U)):\n",
        "        for j in range(0, len(U[0])):\n",
        "            if abs(U[i][j] - U_old[i][j]) > epsm:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def cal_U(data, C, cluster_number, m):\n",
        "    U=[0.0,0.0]\n",
        "    distance_matrix = [0.0,0.0]\n",
        "    #current = []\n",
        "    for j in range(0, cluster_number):\n",
        "       #print('cluster_j:',j)\n",
        "       distance_matrix[j]=distance(data,C[j])\n",
        "       #current.append(distance(data, C[j]))\n",
        "       #distance_matrix.append(current)\n",
        "    # 更新U\n",
        "    for j in range(0, cluster_number):\n",
        "        dummy = 0.0\n",
        "        for k in range(0, cluster_number):\n",
        "            dummy += (distance_matrix[j] / distance_matrix[k]) ** (2 / (m - 1))\n",
        "        U[j] = 1 / dummy\n",
        "    return U\n",
        "def updata_U(checkpoints_dir, data, U, UC_name):\n",
        "    cluster_number = len(U[0])\n",
        "\n",
        "    for iter in range(100):\n",
        "        print('第%d 次迭代更新U' % iter)\n",
        "        U_old = copy.deepcopy(U)\n",
        "        C = []\n",
        "        for j in range(0, cluster_number):\n",
        "            current_cluster_center = []\n",
        "            for i in range(0, len(data[0])):\n",
        "                dummy_sum_num = 0.0\n",
        "                dummy_sum_dum = 0.0\n",
        "                for k in range(0, len(data)):\n",
        "                    dummy_sum_num += (U[k][j] ** m) * data[k][i]\n",
        "                    dummy_sum_dum += (U[k][j] ** m)\n",
        "                current_cluster_center.append(dummy_sum_num / dummy_sum_dum)\n",
        "            C.append(current_cluster_center)\n",
        "\n",
        "        distance_matrix = []\n",
        "        for i in range(0, len(data)):\n",
        "            current = []\n",
        "            for j in range(0, cluster_number):\n",
        "                current.append(distance(data[i], C[j]))\n",
        "            distance_matrix.append(current)\n",
        "        # 更新U\n",
        "        for j in range(0, cluster_number):\n",
        "            for i in range(0, len(data)):\n",
        "                dummy = 0.0\n",
        "                for k in range(0, cluster_number):\n",
        "                    dummy += (distance_matrix[i][j] / distance_matrix[i][k]) ** (2 / (m - 1))\n",
        "                U[i][j] = 1 / dummy\n",
        "\n",
        "        if end_conditon(U, U_old):\n",
        "             print(\"结束聚类\")\n",
        "             break\n",
        "\n",
        "    np.savetxt(checkpoints_dir + \"/Uy\" + UC_name + '.txt', U, fmt=\"%.20f\", delimiter=\",\")\n",
        "    np.savetxt(checkpoints_dir + \"/Cy\" + UC_name + '.txt', C, fmt=\"%.20f\", delimiter=\",\")\n",
        "\n",
        "    return U, C\n",
        "\n",
        "\n",
        "def getSubU(U, idx):\n",
        "    target = []\n",
        "    for i in idx:\n",
        "        target.append(U[i])\n",
        "\n",
        "    return target\n",
        "\n",
        "def initialize_UC_test(x_len,x_data,y_len,y_data, UC_name,checkpoints_dir):\n",
        "    #x_images, x_id_list, x_len, x_labels = dr.get_source_batch(0, 256, 256, source_dir=source_dir)\n",
        "    #y_images, y_id_list, y_len, y_labels = dr.get_target_batch(0, 256, 256, target_dir=target_dir)\n",
        "    #print('x_len',len(x_images))\n",
        "    #print('y_len',len(y_images))\n",
        "    #x = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
        "    #y = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
        "\n",
        "    # C_initial = Classifier('C', True, reuse=True)\n",
        "    #fx = C_initial(x)\n",
        "    #fy = C_initial(y)\n",
        "\n",
        "    #with tf.Session() as sess:\n",
        "    #    tf.global_variables_initializer().run()\n",
        "\n",
        "    #    x_data = []\n",
        "    #    y_data = []\n",
        "\n",
        "    #    for img in x_images:\n",
        "    #        data = sess.run(fx, feed_dict={x: [img]})\n",
        "    #        x_data.append(data[0])\n",
        "    #    for img in y_images:\n",
        "    #        data = sess.run(fy, feed_dict={y: [img]})\n",
        "    #        y_data.append(data[0])\n",
        "    #print('y_data:',np.sum(y_data,axis=1))\n",
        "    Ux = initialize_U(x_len, 1)\n",
        "    Uy = initialize_U(y_len, 2)\n",
        "    Cx = initialize_C(x_data, Ux)\n",
        "    Cy = initialize_C(y_data, Uy)\n",
        "    print('Cx:',np.shape(Cx))\n",
        "    print('Cy:',np.shape(Cy))\n",
        "\n",
        "    return Ux, Uy, Cx, Cy\n",
        "def initialize_UC(checkpoints_dir, C_initial, UC_name, source_dir, target_dir):\n",
        "    x_images, x_id_list, x_len, x_labels = dr.get_source_batch(0, 256, 256, source_dir=source_dir)\n",
        "    y_images, y_id_list, y_len, y_labels = dr.get_target_batch(0, 256, 256, target_dir=target_dir)\n",
        "    #print('x_len',len(x_images))\n",
        "    #print('y_len',len(y_images))\n",
        "    x = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
        "    y = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
        "\n",
        "    # C_initial = Classifier('C', True, reuse=True)\n",
        "    fx = C_initial(x)\n",
        "    fy = C_initial(y)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "\n",
        "        for img in x_images:\n",
        "            data = sess.run(fx, feed_dict={x: [img]})\n",
        "            x_data.append(data[0])\n",
        "        for img in y_images:\n",
        "            data = sess.run(fy, feed_dict={y: [img]})\n",
        "            y_data.append(data[0])\n",
        "    print('y_data:',np.sum(y_data,axis=1))\n",
        "    Ux = initialize_U(x_len, 1)\n",
        "    Uy = initialize_U(y_len, 2)\n",
        "    Cx = initialize_C(x_data, Ux)\n",
        "    Cy = initialize_C(y_data, Uy)\n",
        "\n",
        "    np.savetxt(checkpoints_dir + \"/Ux\" + UC_name + '.txt', Ux, fmt=\"%.20f\", delimiter=\",\")\n",
        "    np.savetxt(checkpoints_dir + \"/Uy\" + UC_name + '.txt', Uy, fmt=\"%.20f\", delimiter=\",\")\n",
        "    np.savetxt(checkpoints_dir + \"/Cx\" + UC_name + '.txt', Cx, fmt=\"%.20f\", delimiter=\",\")\n",
        "    np.savetxt(checkpoints_dir + \"/Cy\" + UC_name + '.txt', Cy, fmt=\"%.20f\", delimiter=\",\")\n",
        "\n",
        "    return Ux, Uy, Cx, Cy, x_data, y_data, y_labels\n",
        "\n",
        "\n",
        "def initialize_UC2(C_initial, y_images):\n",
        "    # y_images, y_id_list, y_len, y_labels = dr.get_target_batch(0, 256, 256, \".png\", target_dir=target_dir)\n",
        "\n",
        "    y = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
        "\n",
        "    fy = C_initial(y)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        y_data = []\n",
        "\n",
        "        for img in y_images:\n",
        "            data = sess.run(fy, feed_dict={y: [img]})\n",
        "            y_data.append(data[0])\n",
        "\n",
        "    return y_data\n",
        "\n",
        "\n",
        "def get_nearst_accuracy(datas, C, label):\n",
        "    distances = []\n",
        "    for data in datas:\n",
        "        distance = []\n",
        "        distance_1 = np.mean(np.sqrt(np.square(np.subtract(data, C[0]))))\n",
        "        distance_2 = np.mean(np.sqrt(np.square(np.subtract(data, C[1]))))\n",
        "        distance.append(distance_1)\n",
        "        distance.append(distance_2)\n",
        "        distances.append(distance)\n",
        "\n",
        "    accuracy = 0\n",
        "    wrong = 0\n",
        "    wrong_cell = 0\n",
        "    num_cell = 0\n",
        "    wrong_toxo = 0\n",
        "    num_toxo = 0\n",
        "    for index in range(len(distances)):\n",
        "        arg = np.argmin(distances[index])\n",
        "        if label[index] == 0:\n",
        "            num_toxo += 1\n",
        "        else:\n",
        "            num_cell += 1\n",
        "        if arg == label[index]:\n",
        "            accuracy += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "            if label[index] == 0:\n",
        "                wrong_toxo += 1\n",
        "            else:\n",
        "                wrong_cell += 1\n",
        "\n",
        "    return accuracy / len(datas), wrong_toxo / num_toxo, wrong_cell / num_cell, num_toxo, num_cell\n",
        "if __name__ == '__main__':\n",
        "    # initialize_UC()\n",
        "    pass"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shyQ-lzREWI7"
      },
      "source": [
        "import data_reader as dr\n",
        "import fuzzy\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "from compute_accuracy import computeAccuracy\n",
        "\n",
        "m = 2\n",
        "def initialize_U(num_data, num_cluster):\n",
        "    U = []\n",
        "    for data in range(num_data):\n",
        "        u_data = []\n",
        "        sum_random = 0.0\n",
        "        for cluster in range(num_cluster):\n",
        "            u_data.append(random.random())\n",
        "            sum_random += u_data[-1]\n",
        "        for cluster in range(num_cluster):\n",
        "            u_data[cluster] /= sum_random\n",
        "        U.append(u_data)\n",
        "\n",
        "    return U\n",
        "\n",
        "\n",
        "def initialize_C(data, U):\n",
        "    \"\"\"\n",
        "    :param data: features of the data\n",
        "    :param U:\n",
        "    :param m:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_cluster = len(U[0])\n",
        "    C = []\n",
        "    for j in range(num_cluster):\n",
        "        current_cluster_center = []\n",
        "        for i in range(len(data[0])):\n",
        "            dummy_sum_num = 0.0\n",
        "            dummy_sum_dum = 0.0\n",
        "            for k in range(len(data)):\n",
        "                # 分子\n",
        "                dummy_sum_num += (U[k][j] ** m) * data[k][i]\n",
        "                # 分母\n",
        "                dummy_sum_dum += (U[k][j] ** m)\n",
        "            # 第i列的聚类中心\n",
        "            current_cluster_center.append(dummy_sum_num / dummy_sum_dum)\n",
        "            # 第j簇的所有聚类中心\n",
        "        C.append(current_cluster_center)\n",
        "    return C\n",
        "def initialize_UC(y_data):\n",
        "    print('y_data:',np.sum(y_data,axis=1))\n",
        "    y_len=len(y_data)\n",
        "    #Ux = initialize_U(x_len, 1)\n",
        "    Uy = initialize_U(y_len, 2)\n",
        "    #Cx = initialize_C(x_data, Ux)\n",
        "    Cy = initialize_C(y_data, Uy)\n",
        "    #print('Cx:',np.shape(Cx))\n",
        "    print('Cy:',np.shape(Cy))\n",
        "    return Uy,Cy\n",
        "def end_conditon(U, U_old):\n",
        "    \"\"\"\n",
        "    结束条件。当U矩阵随着连续迭代停止变化时，触发结束\n",
        "    \"\"\"\n",
        "    epsm = 0.000000001\n",
        "    for i in range(0, len(U)):\n",
        "        for j in range(0, len(U[0])):\n",
        "            if abs(U[i][j] - U_old[i][j]) > epsm:\n",
        "                return False\n",
        "    return True\n",
        "def calculate_J(data, U, C):\n",
        "    \"\"\"\n",
        "    计算目标函数J\n",
        "    :param data:\n",
        "    :param C:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    J = 0.0\n",
        "    for num_C in range(len(C)):\n",
        "        for num_data in range(len(data)):\n",
        "            J += (np.power(U[num_data][num_C], m) * np.square(np.subtract(C[num_C], data[num_data])))\n",
        "    return J\n",
        "\n",
        "\n",
        "def distance(point, center):\n",
        "    \"\"\"\n",
        "    该函数计算2点之间的距离（作为列表）。我们指欧几里德距离。        闵可夫斯基距离\n",
        "    \"\"\"\n",
        "    if len(point) != len(center):\n",
        "        return -1\n",
        "    dummy = 0.0\n",
        "    for i in range(0, len(point)):\n",
        "        dummy += abs(point[i] - center[i]) ** 2\n",
        "    return math.sqrt(dummy)\n",
        "\n",
        "def updata_U(saveDir, data, U):\n",
        "    cluster_number = len(U[0])\n",
        "\n",
        "    for iter in range(100):\n",
        "        print('第%d 次迭代更新U' % iter)\n",
        "        U_old = copy.deepcopy(U)\n",
        "        C = []\n",
        "        for j in range(0, cluster_number):\n",
        "            current_cluster_center = []\n",
        "            for i in range(0, len(data[0])):\n",
        "                dummy_sum_num = 0.0\n",
        "                dummy_sum_dum = 0.0\n",
        "                for k in range(0, len(data)):\n",
        "                    dummy_sum_num += (U[k][j] ** m) * data[k][i]\n",
        "                    dummy_sum_dum += (U[k][j] ** m)\n",
        "                current_cluster_center.append(dummy_sum_num / dummy_sum_dum)\n",
        "            C.append(current_cluster_center)\n",
        "\n",
        "        distance_matrix = []\n",
        "        for i in range(0, len(data)):\n",
        "            current = []\n",
        "            for j in range(0, cluster_number):\n",
        "                current.append(distance(data[i], C[j]))\n",
        "            distance_matrix.append(current)\n",
        "        # 更新U\n",
        "        for j in range(0, cluster_number):\n",
        "            for i in range(0, len(data)):\n",
        "                dummy = 0.0\n",
        "                for k in range(0, cluster_number):\n",
        "                    dummy += (distance_matrix[i][j] / distance_matrix[i][k]) ** (2 / (m - 1))\n",
        "                U[i][j] = 1 / dummy\n",
        "\n",
        "        if end_conditon(U, U_old):\n",
        "             print(\"结束聚类\")\n",
        "             break\n",
        "\n",
        "    np.savetxt(saveDir + \"/U.txt\", U, fmt=\"%.20f\", delimiter=\",\")\n",
        "    np.savetxt(saveDir + \"/C.txt\", C, fmt=\"%.20f\", delimiter=\",\")\n",
        "    return U, C\n",
        "if __name__=='__main__':\n",
        "    #x_path=\"/home/root123/data/datasets/source/banana/\"\n",
        "    model='20190416-2037'\n",
        "    dir='./checkpoints/+'+model+'/max/'\n",
        "    y_path=\"/home/root123/data/datasets/target/toxo40/\"\n",
        "    #x_images, x_id_list, x_len, x_labels = dr.get_source_batch(0, 256, 256, source_dir=x_path)\n",
        "    y_images, y_id_list, y_len, y_labels = dr.get_target_batch(0, 256, 256, target_dir=y_path)\n",
        "\n",
        "    feature=np.load(dir+'feature_fcgan.npy')\n",
        "    Uy,Cy= initialize_UC(feature)\n",
        "    U,C=updata_U(dir,feature,Uy)\n",
        "    print(np.shape(U))\n",
        "    print(np.shape(C))\n",
        "    \n",
        "    y, y_idx_list, y_data_n, y_labels = dr.get_target_batch(0, 224, 224, target_dir=y_path)\n",
        "    #Uy = np.loadtxt('checkpoints/' + model + '/Uy' + UC_name + '.txt', delimiter=\",\")\n",
        "    #Uy = np.loadtxt(dir+'/U.txt', delimiter=\",\")\n",
        "    accuracy=computeAccuracy(U,y_labels,0.5)\n",
        "    print('accuracy:',accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI6Bm84REjI0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, name, is_training, ngf=64, norm='instance', image_size=128):\n",
        "        self.name = name\n",
        "        self.reuse = False\n",
        "        self.ngf = ngf\n",
        "        self.norm = norm\n",
        "        self.is_training = is_training\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __call__(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          input: batch_size x width x height x 3\n",
        "        Returns:\n",
        "          output: same size as input\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            # conv layers\n",
        "            c7s1_32 = ops.c7s1_k(input, self.ngf, is_training=self.is_training, norm=self.norm,\n",
        "                                 reuse=self.reuse, name='c7s1_32')  # (?, w, h, 32)\n",
        "            d64 = ops.dk(c7s1_32, 2 * self.ngf, is_training=self.is_training, norm=self.norm,\n",
        "                         reuse=self.reuse, name='d64')  # (?, w/2, h/2, 64)\n",
        "            d128 = ops.dk(d64, 4 * self.ngf, is_training=self.is_training, norm=self.norm,\n",
        "                          reuse=self.reuse, name='d128')  # (?, w/4, h/4, 128)\n",
        "\n",
        "            if self.image_size <= 128:\n",
        "                # use 6 residual blocks for 128x128 images\n",
        "                res_output = ops.n_res_blocks(d128, reuse=self.reuse, n=6)  # (?, w/4, h/4, 128)\n",
        "            else:\n",
        "                # 9 blocks for higher resolution\n",
        "                res_output = ops.n_res_blocks(d128, reuse=self.reuse, n=9)  # (?, w/4, h/4, 128)\n",
        "\n",
        "            # fractional-strided convolution\n",
        "            u64 = ops.uk(res_output, 2 * self.ngf, is_training=self.is_training, norm=self.norm,\n",
        "                         reuse=self.reuse, name='u64')  # (?, w/2, h/2, 64)\n",
        "            u32 = ops.uk(u64, self.ngf, is_training=self.is_training, norm=self.norm,\n",
        "                         reuse=self.reuse, name='u32', output_size=self.image_size)  # (?, w, h, 32)\n",
        "\n",
        "            # conv layer\n",
        "            # Note: the paper said that ReLU and _norm were used\n",
        "            # but actually tanh was used and no _norm here\n",
        "            output = ops.c7s1_k(u32, 3, norm=None,\n",
        "                                activation='tanh', reuse=self.reuse, name='output')  # (?, w, h, 3)\n",
        "        # set reuse=True for next call\n",
        "        self.reuse = True\n",
        "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def sample(self, input):\n",
        "        image = utils.batch_convert2int(self.__call__(input))\n",
        "        image = tf.image.encode_jpeg(tf.squeeze(image, [0]))\n",
        "        return image"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuI5CSAOE1oG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class CycleGAN:\n",
        "    def __init__(self,\n",
        "                 batch_size=1,\n",
        "                 image_size=256,\n",
        "                 use_lsgan=True,\n",
        "                 norm='instance',\n",
        "                 lambda1=10,\n",
        "                 lambda2=10,\n",
        "                 learning_rate=2e-4,\n",
        "                 learning_rate2=2e-6,\n",
        "                 beta1=0.5,\n",
        "                 ngf=64\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          batch_size: integer, batch size\n",
        "          image_size: integer, image size\n",
        "          lambda1: integer, weight for forward cycle loss (X->Y->X)\n",
        "          lambda2: integer, weight for backward cycle loss (Y->X->Y)\n",
        "          use_lsgan: boolean\n",
        "          norm: 'instance' or 'batch'\n",
        "          learning_rate: float, initial learning rate for Adam\n",
        "          beta1: float, momentum term of Adam\n",
        "          ngf: number of gen filters in first conv layer\n",
        "        \"\"\"\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "        self.use_lsgan = use_lsgan\n",
        "        use_sigmoid = not use_lsgan\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.learning_rate2 = learning_rate2\n",
        "        self.beta1 = beta1\n",
        "\n",
        "        self.is_training = tf.placeholder_with_default(True, shape=[], name='is_training')\n",
        "\n",
        "        self.G = Generator('G', self.is_training, ngf=ngf, norm=norm, image_size=image_size)\n",
        "        self.D_Y = Discriminator('D_Y',\n",
        "                                 self.is_training, norm=norm, use_sigmoid=use_sigmoid)\n",
        "        self.F = Generator('F', self.is_training, norm=norm, image_size=image_size)\n",
        "        self.D_X = Discriminator('D_X',\n",
        "                                 self.is_training, norm=norm, use_sigmoid=use_sigmoid)\n",
        "        self.C = Classifier(\"C\", self.is_training, norm=norm, use_sigmoid=use_sigmoid)\n",
        "\n",
        "        self.Uy2x = tf.placeholder(tf.float32, [None, 2], name=\"Uy2x\")\n",
        "        self.Ux2y = tf.placeholder(tf.float32, [None, 1], name=\"Ux2y\")\n",
        "        self.x_label = tf.placeholder(tf.float32, [None], name=\"x_label\")\n",
        "        self.y_label = tf.placeholder(tf.float32, [None], name=\"y_label\")\n",
        "        self.ClusterX = tf.placeholder(tf.float32, [1, 100], name=\"ClusterX\")\n",
        "        self.ClusterY = tf.placeholder(tf.float32, [2, 100], name=\"ClusterY\")\n",
        "\n",
        "        self.x = tf.placeholder(tf.float32,\n",
        "                                shape=[batch_size, image_size, image_size, 3], name=\"x\")\n",
        "        self.y = tf.placeholder(tf.float32,\n",
        "                                shape=[batch_size, image_size, image_size, 3], name=\"y\")\n",
        "        #self.fake_x=self.F(self.y)\n",
        "        #self.fake_y=self.G(self.x)\n",
        "        #self.fake_x = tf.placeholder(tf.float32,\n",
        "        #                             shape=[batch_size, image_size, image_size, 3], name=\"fake_x\")\n",
        "        #self.fake_y = tf.placeholder(tf.float32,\n",
        "        #                             shape=[batch_size, image_size, image_size, 3], name=\"fake_y\")\n",
        "\n",
        "    def model(self):\n",
        "        x = self.x\n",
        "        y = self.y\n",
        "\n",
        "        #self.fake_x = self.F(self.y)\n",
        "        #self.fake_y = self.G(self.x)\n",
        "        cycle_loss = self.cycle_consistency_loss(self.G, self.F, x, y)\n",
        "\n",
        "        # X -> Y\n",
        "        fake_y = self.G(x)\n",
        "        G_gan_loss = self.generator_loss(self.D_Y, fake_y, self.y_label, use_lsgan=self.use_lsgan)\n",
        "        G_loss = G_gan_loss + cycle_loss\n",
        "        D_Y_loss = self.discriminator_loss(self.D_Y, y, fake_y, self.y_label, use_lsgan=self.use_lsgan)\n",
        "\n",
        "        # Y -> X\n",
        "        fake_x = self.F(y)\n",
        "        F_gan_loss = self.generator_loss(self.D_X, fake_x, self.x_label, use_lsgan=self.use_lsgan)\n",
        "        F_loss = F_gan_loss + cycle_loss\n",
        "        D_X_loss = self.discriminator_loss(self.D_X, x, fake_x, self.x_label, use_lsgan=self.use_lsgan)\n",
        "\n",
        "        # fuzzy\n",
        "        Fuzzy_x_loss, feature_x = self.fuzzy_loss(self.C, x, self.Ux2y, self.ClusterX)\n",
        "        Fuzzy_y_loss,feature_y = self.fuzzy_loss(self.C, fake_x, self.Uy2x, self.ClusterY)\n",
        "        Disperse_loss = -self.disperse_loss(feature_y, self.Uy2x)\n",
        "        Fuzzy_loss = Fuzzy_x_loss + Fuzzy_y_loss#+Disperse_loss\n",
        "\n",
        "        #feature_x = self.C(x)\n",
        "        #feature_y = self.C(fake_x)\n",
        "        # summary\n",
        "        tf.summary.histogram('D_Y/true', self.D_Y(y))\n",
        "        tf.summary.histogram('D_Y/fake', self.D_Y(self.G(x)))\n",
        "        tf.summary.histogram('D_X/true', self.D_X(x))\n",
        "        tf.summary.histogram('D_X/fake', self.D_X(self.F(y)))\n",
        "\n",
        "        tf.summary.scalar('loss/G', G_gan_loss)\n",
        "        tf.summary.scalar('loss/D_Y', D_Y_loss)\n",
        "        tf.summary.scalar('loss/F', F_gan_loss)\n",
        "        tf.summary.scalar('loss/D_X', D_X_loss)\n",
        "        tf.summary.scalar('loss/cycle', cycle_loss)\n",
        "        tf.summary.scalar('loss/Disperse', Disperse_loss)\n",
        "        tf.summary.scalar('loss/Fuzzy', Fuzzy_loss)\n",
        "\n",
        "        tf.summary.image('X/generated', utils.batch_convert2int(self.G(x)))\n",
        "        tf.summary.image('X/reconstruction', utils.batch_convert2int(self.F(self.G(x))))\n",
        "        tf.summary.image('Y/generated', utils.batch_convert2int(self.F(y)))\n",
        "        tf.summary.image('Y/reconstruction', utils.batch_convert2int(self.G(self.F(y))))\n",
        "\n",
        "        return G_loss, D_Y_loss, F_loss, D_X_loss, fake_y, fake_x, Disperse_loss, Fuzzy_loss,feature_x,feature_y\n",
        "\n",
        "    def optimize(self, G_loss, D_Y_loss, F_loss, D_X_loss, Disperse_loss):\n",
        "        def make_optimizer(loss, variables, name='Adam'):\n",
        "            \"\"\" Adam optimizer with learning rate 0.0002 for the first 100k steps (~100 epochs)\n",
        "                and a linearly decaying rate that goes to zero over the next 100k steps\n",
        "            \"\"\"\n",
        "            global_step = tf.Variable(0, trainable=False)\n",
        "            starter_learning_rate = self.learning_rate\n",
        "            end_learning_rate = 0.0\n",
        "            start_decay_step = 100000\n",
        "            decay_steps = 100000\n",
        "            beta1 = self.beta1\n",
        "            learning_rate = (\n",
        "                tf.where(\n",
        "                    tf.greater_equal(global_step, start_decay_step),\n",
        "                    tf.train.polynomial_decay(starter_learning_rate, global_step - start_decay_step,\n",
        "                                              decay_steps, end_learning_rate,\n",
        "                                              power=1.0),\n",
        "                    starter_learning_rate\n",
        "                )\n",
        "            )\n",
        "            tf.summary.scalar('learning_rate/{}'.format(name), learning_rate)\n",
        "\n",
        "            learning_step = (\n",
        "                tf.train.AdamOptimizer(learning_rate, beta1=beta1, name=name)\n",
        "                    .minimize(loss, global_step=global_step, var_list=variables)\n",
        "            )\n",
        "            return learning_step\n",
        "\n",
        "        G_optimizer = make_optimizer(G_loss, self.G.variables, name='Adam_G')\n",
        "        D_Y_optimizer = make_optimizer(D_Y_loss, self.D_Y.variables, name='Adam_D_Y')\n",
        "        F_optimizer = make_optimizer(F_loss, self.F.variables, name='Adam_F')\n",
        "        D_X_optimizer = make_optimizer(D_X_loss, self.D_X.variables, name='Adam_D_X')\n",
        "        Disperse_optimizer = make_optimizer(Disperse_loss, self.C.variables, name='Adam_D_X')\n",
        "\n",
        "        #with tf.control_dependencies([G_optimizer, D_Y_optimizer, F_optimizer, D_X_optimizer, Disperse_optimizer]):\n",
        "        #    return tf.no_op(name='optimizers')\n",
        "        with tf.control_dependencies([G_optimizer, D_Y_optimizer, F_optimizer, D_X_optimizer]):\n",
        "            return tf.no_op(name='optimizers')\n",
        "    def optimize2(self, Fuzzy_loss):\n",
        "        def make_optimizer2(loss, variables, name='Adam2'):\n",
        "            \"\"\" Adam optimizer with learning rate 0.0002 for the first 100k steps (~100 epochs)\n",
        "                and a linearly decaying rate that goes to zero over the next 100k steps\n",
        "            \"\"\"\n",
        "            global_step = tf.Variable(0, trainable=False)\n",
        "            starter_learning_rate = self.learning_rate2\n",
        "            end_learning_rate = 0.0\n",
        "            start_decay_step = 100000\n",
        "            decay_steps = 100000\n",
        "            beta1 = self.beta1\n",
        "            learning_rate = (\n",
        "                tf.where(\n",
        "                    tf.greater_equal(global_step, start_decay_step),\n",
        "                    tf.train.polynomial_decay(starter_learning_rate, global_step - start_decay_step,\n",
        "                                              decay_steps, end_learning_rate,\n",
        "                                              power=1.0),\n",
        "                    starter_learning_rate\n",
        "                )\n",
        "            )\n",
        "            tf.summary.scalar('learning_rate2/{}'.format(name), learning_rate)\n",
        "\n",
        "            learning_step = (\n",
        "                tf.train.AdamOptimizer(learning_rate, beta1=beta1, name=name)\n",
        "                    .minimize(loss, global_step=global_step, var_list=variables)\n",
        "            )\n",
        "            return learning_step\n",
        "\n",
        "        Fuzzy_optimizer = make_optimizer2(Fuzzy_loss, self.C.variables, name='Adam_Fuzzy')\n",
        "\n",
        "        with tf.control_dependencies([Fuzzy_optimizer]):\n",
        "            return tf.no_op(name='optimizers2')\n",
        "\n",
        "    def discriminator_loss(self, D, y, fake_y, label, use_lsgan=True):\n",
        "        \"\"\" Note: default: D(y).shape == (batch_size,5,5,1),\n",
        "                           fake_buffer_size=50, batch_size=1\n",
        "        Args:\n",
        "          G: generator object\n",
        "          D: discriminator object\n",
        "          y: 4D tensor (batch_size, image_size, image_size, 3)\n",
        "        Returns:\n",
        "          loss: scalar\n",
        "        \"\"\"\n",
        "        if use_lsgan:\n",
        "            # use mean squared error\n",
        "            error_real = tf.reduce_mean(tf.squared_difference(D(y), label))\n",
        "            error_fake = tf.reduce_mean(tf.square(D(fake_y)))\n",
        "        else:\n",
        "            # use cross entropy\n",
        "            error_real = -tf.reduce_mean(ops.safe_log(D(y)))\n",
        "            error_fake = -tf.reduce_mean(ops.safe_log(1 - D(fake_y)))\n",
        "        loss = (error_real + error_fake) / 2\n",
        "        return loss\n",
        "\n",
        "    def generator_loss(self, D, fake_y, label, use_lsgan=True):\n",
        "        \"\"\"  fool discriminator into believing that G(x) is real\n",
        "        \"\"\"\n",
        "        if use_lsgan:\n",
        "            # use mean squared error\n",
        "            loss = tf.reduce_mean(tf.squared_difference(D(fake_y), label))\n",
        "        else:\n",
        "            # heuristic, non-saturating loss\n",
        "            loss = -tf.reduce_mean(ops.safe_log(D(fake_y))) / 2\n",
        "        return loss\n",
        "\n",
        "    def cycle_consistency_loss(self, G, F, x, y):\n",
        "        \"\"\" cycle consistency loss (L1 norm)\n",
        "        \"\"\"\n",
        "        forward_loss = tf.reduce_mean(tf.abs(F(G(x)) - x))\n",
        "        backward_loss = tf.reduce_mean(tf.abs(G(F(y)) - y))\n",
        "        loss = self.lambda1 * forward_loss + self.lambda2 * backward_loss\n",
        "        return loss\n",
        "\n",
        "    def disperse_loss(self, data, U, DIM=100, m=2):\n",
        "        data_n = self.batch_size\n",
        "        cluster_num = 2\n",
        "        tensor_m = tf.constant(np.ones([data_n, cluster_num], dtype=np.float32) * m)\n",
        "        UM = tf.pow(U, tensor_m)\n",
        "        dumpy_sum_num = tf.matmul(UM, data, transpose_a=True)\n",
        "        dum = tf.expand_dims(tf.reduce_sum(UM, 0), 1)\n",
        "        g = []\n",
        "        for i in range(DIM):\n",
        "            g.append(dum)\n",
        "        dumpy_sum_dum = tf.concat(g, axis=1)\n",
        "        clusters = tf.divide(dumpy_sum_num, dumpy_sum_dum)\n",
        "        c1 = []\n",
        "        c2 = []\n",
        "        for i in range(data_n):\n",
        "            c1.append(tf.expand_dims(clusters[0], 0))\n",
        "            c2.append(tf.expand_dims(clusters[1], 0))\n",
        "        cluster_1 = tf.concat(c1, axis=0)\n",
        "        cluster_2 = tf.concat(c2, axis=0)\n",
        "        distance = tf.reduce_mean(tf.sqrt(tf.pow(tf.subtract(cluster_1, cluster_2), 2)))\n",
        "        return distance\n",
        "    def disperse_loss_original(self, data, C, U, DIM=100, m=2):\n",
        "        data_n = self.batch_size\n",
        "        cluster_num = 2\n",
        "        tensor_m = tf.constant(np.ones([data_n, cluster_num], dtype=np.float32) * m)\n",
        "        UM = tf.pow(U, tensor_m)\n",
        "        dumpy_sum_num = tf.matmul(UM, C(data), transpose_a=True)\n",
        "        dum = tf.expand_dims(tf.reduce_sum(UM, 0), 1)\n",
        "        g = []\n",
        "        for i in range(DIM):\n",
        "            g.append(dum)\n",
        "        dumpy_sum_dum = tf.concat(g, axis=1)\n",
        "        clusters = tf.divide(dumpy_sum_num, dumpy_sum_dum)\n",
        "        c1 = []\n",
        "        c2 = []\n",
        "        for i in range(data_n):\n",
        "            c1.append(tf.expand_dims(clusters[0], 0))\n",
        "            c2.append(tf.expand_dims(clusters[1], 0))\n",
        "        cluster_1 = tf.concat(c1, axis=0)\n",
        "        cluster_2 = tf.concat(c2, axis=0)\n",
        "        distance = tf.reduce_mean(tf.sqrt(tf.pow(tf.subtract(cluster_1, cluster_2), 2)))\n",
        "        return distance\n",
        "\n",
        "    def fuzzy_loss(self, C, x, U, clusters):\n",
        "        data = C(x)\n",
        "        data_n = data.get_shape()[0]\n",
        "        c1 = []\n",
        "        c2 = []\n",
        "        for i in range(data_n):\n",
        "            c1.append(tf.expand_dims(clusters[0], 0))\n",
        "            if clusters.get_shape()[0] == 2:\n",
        "                c2.append(tf.expand_dims(clusters[1], 0))\n",
        "        cluster_1 = tf.concat(c1, axis=0)\n",
        "        if clusters.get_shape()[0] == 2:\n",
        "            cluster_2 = tf.concat(c2, axis=0)\n",
        "\n",
        "        distance_1 = tf.reduce_mean(tf.sqrt(tf.pow(tf.subtract(data, cluster_1), 2)), axis=1)\n",
        "        if clusters.get_shape()[0] == 2:\n",
        "            distance_2 = tf.reduce_mean(tf.sqrt(tf.pow(tf.subtract(data, cluster_2), 2)), axis=1)\n",
        "            distance = tf.concat([tf.expand_dims(distance_1, 1), tf.expand_dims(distance_2, 1)], axis=1)\n",
        "        else:\n",
        "            distance = distance_1\n",
        "        fuzzyLoss = tf.reduce_mean(tf.multiply(distance, U))\n",
        "\n",
        "        return fuzzyLoss,data"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E9YFMp3FSUm"
      },
      "source": [
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "import os\n",
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.cm as cm\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
        "\n",
        "\n",
        "\n",
        "Ux = Uy = Cx = Cy = None\n",
        "\n",
        "x_img_path='/content/drive/MyDrive/Colab Notebook/Data2/banana/train/0_000004.jpg'\n",
        "x_name=x_img_path.split('/')[-1].split('.')[0]\n",
        "y_img_path='/home/root123/data/FCGAN/data/'\n",
        "y_name=y_img_path.split('/')[-1].split('.')[0]\n",
        "\n",
        "def train():\n",
        "    if FLAGS.load_model is not None:\n",
        "        checkpoints_dir = \"checkpoints/\" + FLAGS.load_model.lstrip(\"checkpoints/\")\n",
        "        print(checkpoints_dir)\n",
        "    else:\n",
        "        logging.info('No model to test, stopped!')\n",
        "        return\n",
        "\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        cycle_gan = CycleGAN(\n",
        "            batch_size=FLAGS.batch_size,\n",
        "            image_size=FLAGS.image_size,\n",
        "            use_lsgan=FLAGS.use_lsgan,\n",
        "            norm=FLAGS.norm,\n",
        "            lambda1=FLAGS.lambda1,\n",
        "            lambda2=FLAGS.lambda2,\n",
        "            learning_rate=FLAGS.learning_rate,\n",
        "            learning_rate2=FLAGS.learning_rate2,\n",
        "            beta1=FLAGS.beta1,\n",
        "            ngf=FLAGS.ngf\n",
        "        )\n",
        "        G_loss, D_Y_loss, F_loss, D_X_loss, fake_y, fake_x, Disperse_loss, Fuzzy_loss, feature_x, feature_y = cycle_gan.model()\n",
        "\n",
        "        summary_op = tf.summary.merge_all()\n",
        "        train_writer = tf.summary.FileWriter(checkpoints_dir, graph)\n",
        "        saver = tf.train.Saver()\n",
        "    logging.info('Network Built!')\n",
        "\n",
        "    with tf.Session(graph=graph) as sess:\n",
        "        checkpoint = tf.train.get_checkpoint_state(checkpoints_dir)\n",
        "        meta_graph_path = checkpoint.model_checkpoint_path + \".meta\"\n",
        "        restore = tf.train.import_meta_graph(meta_graph_path)\n",
        "        restore.restore(sess, tf.train.latest_checkpoint(checkpoints_dir))\n",
        "        step = int(meta_graph_path.split(\"-\")[2].split(\".\")[0])\n",
        "        Ux = np.loadtxt(checkpoints_dir + \"/Ux\" + FLAGS.UC_name + '.txt', delimiter=\",\")\n",
        "        Ux = [[x] for x in Ux]\n",
        "        Uy = np.loadtxt(checkpoints_dir + \"/Uy\" + FLAGS.UC_name + '.txt', delimiter=\",\")\n",
        "        Cx = np.loadtxt(checkpoints_dir + \"/Cx\" + FLAGS.UC_name + '.txt', delimiter=\",\")\n",
        "        Cx = [Cx]\n",
        "        Cy = np.loadtxt(checkpoints_dir + \"/Cy\" + FLAGS.UC_name + '.txt', delimiter=\",\")\n",
        "        logging.info('Parameter Initialized!')\n",
        "\n",
        "        #print('Ux',Ux)\n",
        "\n",
        "        coord = tf.train.Coordinator()\n",
        "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "        try:\n",
        "            plots_count=10\n",
        "            tsne_plot_count=1000\n",
        "            result_dir='./result'\n",
        "            fake_dir=os.path.join(result_dir,'fake_xy')\n",
        "            roc_dir=os.path.join(result_dir,'roc_curves')\n",
        "            plot_dir=os.path.join(result_dir,'tsne_pca')\n",
        "            conv_dir=os.path.join(result_dir,'convs')\n",
        "            #occ_dir=os.path.join(result_dir,'occ_test')\n",
        "            #utils.prepare_dir(occ_dir)\n",
        "\n",
        "            x_path = FLAGS.X + FLAGS.UC_name\n",
        "            x_images, x_id_list, x_len, x_labels, oimg_xs, x_files = get_source_batch(0, 256, 256,\n",
        "                                                                                      source_dir=x_path)\n",
        "            y_images, y_id_list, y_len, y_labels, oimg_ys, y_files = get_target_batch(0, 256, 256,\n",
        "                                                                                      target_dir=FLAGS.Y)\n",
        "            #Compute Accuracy, tp, tn, fp, fn, f1_score, recall, precision, specificity#\n",
        "            accuracy, tp, tn, fp, fn, f1_score, recall, precision, specificity=computeAccuracy(Uy,y_labels)\n",
        "            print(\"accuracy:%.4f\\ttp:%d\\ttn:%d\\tfp %d\\tfn:%d\\tf1_score:%.3f\\trecall:%.3f\\tprecision:%.3f\\tspecicity:%.3f\\t\" %\n",
        "                  (accuracy, tp, tn, fp, fn,f1_score, recall, precision, specificity))\n",
        "            #cv2.imshow('201',oimg_ys[201])\n",
        "            #cv 2.waitKey()\n",
        "            #draw ROC curves\n",
        "            '''\n",
        "            print('y_labels:',np.shape(y_labels))\n",
        "            print('y_scores:',np.shape(Uy[:,0]))\n",
        "            fpr,tpr,thresholds=roc_curve(y_labels,Uy[:,1])\n",
        "            roc_auc=auc(fpr,tpr)\n",
        "            plt.plot(fpr, tpr)\n",
        "            plt.xticks(np.arange(0, 1, 0.1))\n",
        "            plt.yticks(np.arange(0, 1, 0.1))\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            # plt.title(\"A simple plot\")\n",
        "            plt.show()\n",
        "            print('fpr:',np.shape(fpr))\n",
        "            print('tpr:', np.shape(tpr))\n",
        "            print('thresholds:', np.shape(thresholds))\n",
        "            '''\n",
        "            # t-SNE and PCA plots#\n",
        "            for j in range(plots_count):\n",
        "                feature_path=os.path.join(checkpoints_dir,'feature_fcgan.npy')\n",
        "                feature=np.load(feature_path)\n",
        "                print('feature:',len(feature))\n",
        "                randIdx = random.sample(range(0, len(y_labels)), tsne_plot_count)\n",
        "                t_features = []\n",
        "                t_labels = []\n",
        "                for i in range(len(randIdx)):\n",
        "                    t_features.append(feature[randIdx[i]])\n",
        "                    t_labels.append(y_labels[randIdx[i]])\n",
        "                # 使用TSNE进行降维处理。从100维降至2维。\n",
        "                tsne = TSNE(n_components=2, learning_rate=100).fit_transform(t_features)\n",
        "                #pca = PCA().fit_transform(t_features)\n",
        "                #设置画布大小\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                #plt.subplot(121)\n",
        "                plt.scatter(tsne[:, 0], tsne[:, 1], c=t_labels)\n",
        "                #plt.subplot(122)\n",
        "                #plt.scatter(pca[:, 0], pca[:, 1], c=t_labels)\n",
        "                plt.colorbar()  # 使用这一句就可以分辨出，颜色对应的类了！神奇啊。\n",
        "                utils.prepare_dir(plot_dir)\n",
        "                plt.savefig(os.path.join(plot_dir,'plot{}.pdf'.format(j)))\n",
        "\n",
        "\n",
        "            for i in range(10):\n",
        "            #if True:\n",
        "            #Cross Domain Image Generation#\n",
        "\n",
        "                x_img,_,x_oimg = get_single_img(x_img_path)\n",
        "                y_path=os.path.join(y_img_path,str(i+1)+'.jpg')\n",
        "                y_img,_,y_oimg=get_single_img(y_path)\n",
        "                fake_y_eval,fake_x_eval, conv_y_eval = sess.run(\n",
        "                [fake_y, fake_x,  tf.get_collection('conv_output')],\n",
        "                feed_dict={cycle_gan.x: x_img, cycle_gan.y: y_img})\n",
        "                #print(np.shape(fake_y_eval))\n",
        "                #print(np.shape(fake_x_eval))\n",
        "                #print(np.shape(conv_y_eval))\n",
        "                plot_fake_xy(fake_y_eval[0], fake_x_eval[0], x_name, str(i+1), x_oimg,y_oimg,fake_dir)\n",
        "                print('processing:',i)\n",
        "\n",
        "            #Feature Map Visualization#\n",
        "\n",
        "                print('conv_len:', len(conv_y_eval))\n",
        "                print('conv_shape:',np.shape(conv_y_eval[0]))\n",
        "                id_y_dir=os.path.join(conv_dir, str(y_name))\n",
        "                #utils.prepare_dir()\n",
        "                for i, c in enumerate(conv_y_eval):\n",
        "                    #conv_i_dir=os.path.join(id_y_dir,'_layer_'+str(i))\n",
        "                    plot_conv_output(c,i,id_y_dir)\n",
        "                #print(os.path.join(id_y_dir, 'y.png'))\n",
        "                cv2.imwrite(os.path.join(id_y_dir, 'y.png'), y_oimg)\n",
        "\n",
        "            #Occlusion Test#\n",
        "            '''\n",
        "            if True:\n",
        "                t_imgs, t_lbs, t_img = get_single_img(t_img_path)\n",
        "                #s_imgs, s_lbs, t_img = get_single_img(s_img_path)\n",
        "                width=np.shape(t_imgs[0])[0]\n",
        "                height=np.shape(t_imgs[0])[1]\n",
        "                #print('width:',width)\n",
        "                #print('height:', height)\n",
        "                data=generate_occluded_imageset(t_imgs[0],width=width,height=height,occluded_size=16)\n",
        "                #print(data.shape[0])\n",
        "                #print('Cy:',Cy)\n",
        "                u_ys=np.empty([data.shape[0]],dtype='float64')\n",
        "                occ_map=np.empty((width,height),dtype='float64')\n",
        "                print(occ_map.shape)\n",
        "                cnt=0\n",
        "                feature_y_eval = sess.run(\n",
        "                    feature_y,\n",
        "                    feed_dict={cycle_gan.y: [data[0]]})\n",
        "                # print(feature_y_eval)\n",
        "                idx_u = 0\n",
        "                u_y0 = cal_U(feature_y_eval[0], Cy, 2, 2)[idx_u]\n",
        "                occ_value=0\n",
        "                print('u_y0:',u_y0)\n",
        "                for i in range(width):\n",
        "                    print('collum idx:',i)\n",
        "                    print(str(cnt) + ':' + str(occ_value))\n",
        "                    for j in range(height):\n",
        "                        feature_y_eval = sess.run(\n",
        "                            feature_y,\n",
        "                            feed_dict={cycle_gan.y: [data[cnt+1]]})\n",
        "                        # print(feature_y_eval)\n",
        "                        u_y = cal_U(feature_y_eval[0], Cy, 2, 2)[idx_u]\n",
        "                        #print('u_y0:', u_y0)\n",
        "                        #print('u_y:',u_y)\n",
        "                        occ_value=u_y0-u_y\n",
        "                        occ_map[i,j]=occ_value\n",
        "                        #print(str(cnt)+':'+str(occ_value))\n",
        "                        cnt+=1\n",
        "                occ_map_path=os.path.join(occ_dir,'occlusion_map.txt')\n",
        "                np.savetxt(occ_map_path, occ_map, fmt='%0.20f')\n",
        "                cv2.imwrite(os.path.join(occ_dir, 'occ_test.png'), oimg_ys[id_y])\n",
        "                draw_heatmap(occ_map_path=occ_map_path,ori_img=t_img,save_dir=os.path.join(occ_dir,'heatmap.png'))\n",
        "            '''\n",
        "\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logging.info('Interrupted')\n",
        "            coord.request_stop()\n",
        "        except Exception as e:\n",
        "            coord.request_stop(e)\n",
        "        finally:\n",
        "            #save_path = saver.save(sess, checkpoints_dir + \"/model.ckpt\", global_step=step)\n",
        "            #np.savetxt(checkpoints_dir + \"/Uy\" + FLAGS.UC_name + '.txt', Uy, fmt=\"%.20f\", delimiter=\",\")\n",
        "            #np.savetxt(checkpoints_dir + \"/Cy\" + FLAGS.UC_name + '.txt', Cy, fmt=\"%.20f\", delimiter=\",\")\n",
        "            #np.savetxt(checkpoints_dir + \"/Ux\" + FLAGS.UC_name + '.txt', Ux, fmt=\"%.20f\", delimiter=\",\")\n",
        "            #np.savetxt(checkpoints_dir + \"/Cx\" + FLAGS.UC_name + '.txt', Cx, fmt=\"%.20f\", delimiter=\",\")\n",
        "            logging.info(\"stopped\")\n",
        "            # When done, ask the threads to stop.\n",
        "            coord.request_stop()\n",
        "            coord.join(threads)\n",
        "\n",
        "\n",
        "def main(unused_argv):\n",
        "    train()\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGmHRxtHGSO"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, Flatten, Dense, MaxPool2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(MaxPool2D((2,2)))\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
        "model.add(MaxPool2D((2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(), metrics=['acc'])\n",
        "\n",
        "mc = ModelCheckpoint('en_iyi.h5',save_best_only=True,monitor='val_loss',mode='min')\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": []
    }
  ]
}